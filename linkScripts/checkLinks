#!/usr/bin/python
import sys
from urlparse import urlparse
from BeautifulSoup import BeautifulSoup
import urllib2
import re
import requests

def getLinks(url, result, base):
    try:
        # get page
        html_page = urllib2.urlopen(url)
        print url
    except urllib2.HTTPError:
        sys.exit("HTTP error on " + url)
    except urllib2.URLError:
        sys.exit("URL error on " + url)
    except ValueError:
        sys.exit("Value error on " + url)
    except AttributeError:
        sys.exit("Attribute error on " + url)

    soup = BeautifulSoup(html_page)
    links = soup.findAll('a')
    # foreach link in page
    for link in links:
        if base in link.get('href') and link.get('href') not in result and link.get('href') != None:
            result.append(link.get('href'))
            result.extend(getLinks(link.get('href'), result, base))

    return result

# Expecting URL as parameter
if len(sys.argv) != 2:
    print 'File path needed'
    sys.exit(0)

url = sys.argv[1]
base = urlparse(url).netloc
pages = []
getLinks(url, pages, base)
