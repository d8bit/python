#!/usr/bin/python
import sys
import time
from urllib.parse import urlparse
from bs4 import BeautifulSoup
import urllib
import http.client
import re
import requests
import threading
from multiprocessing import Process, Queue

def getLinks(url, pages, base, parentUrl=""):

    try:
        if not isValidLink(url):
            return pages

        # get page
        html_page = urllib.request.urlopen(url).read()
    except http.client.InvalidURL as error:
        print("\n1. " + str(error) + " -> " + url + " (Found in " + parentUrl + ")")
        sys.exit("Invalid URL: " + url)
    except urllib.request.HTTPError as error:
        if 508 != error.getcode() and 504 != error.getcode():
            print("2. " + str(error) + " -> " + url +  " (Found in " + parentUrl + ")")
            sys.exit("HTTP error on " + url)
        return pages
    except urllib.request.URLError as error:
        print("\n3. " + str(error) + " -> " + url + " (Found in " + parentUrl + ")")
        sys.exit("URL error on " + url)
    except ValueError as error:
        # print("\n4. " + str(error) + " -> " + url + " (Found in "+parentUrl+")")
        sys.exit("Value error on " + url)
    except AttributeError as error:
        print("\n5. " + str(error) + " -> " + url + " (Found in " + parentUrl + ")")
        sys.exit("Attribute error on " + url)
    except:
        print("Unknown error on " + url)
        raise

    try:
        soup = BeautifulSoup(html_page, 'html.parser')
    except (UnicodeEncodeError, error):
        # Pdf file
        sys.exit("UnicodeEncodeError error on " + url)


    links = soup.findAll('a')
    # foreach link in page
    for link in links:
        path = link.get('href')
        if path == None:
            continue

        path = formatUrl(path, base)
        path = path.strip()
        domain = urlparse(path).hostname
        pages_content = pages.get()
        if domain != None and domain in base and path not in pages_content and path != None:
            # file = open("./log.txt", "a")
            # file.write(path + "\n")
            pages_content.append(path)
            pages.put(pages_content)
            process = Process(target=getLinks, args=(path,pages,base,url))
            process.start()
            time.sleep(0.1)
        else:
            pages.put(pages_content)

    time.sleep(0.1)
    return pages

def isValidLink(url):
    extensions = ['.jpg', '.png', '.pdf', 'tel:', 'mailto:', 'javascript:']
    for extension in extensions:
        if extension in url:
            return False

    return True

def formatUrl(path, base):
    try:
        if path[0] == '/' and path[1] == '/':
            scheme = urlparse(base).scheme
            path = scheme + ":" + path
        if path[0] == '/':
            path = base + path
    except (IndexError):
        return base
    if not base in path and not 'http' in path:
        path = base + '/' + path
    return path

# Expecting URL as parameter
if len(sys.argv) != 2:
    print('File path needed')
    sys.exit(0)

url = sys.argv[1]
pages = Queue()
pages.put([])

# file = open("./log.txt", "w")
getLinks(url, pages, url)
