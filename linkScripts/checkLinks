#!/usr/bin/python
import sys
from urlparse import urlparse
from BeautifulSoup import BeautifulSoup
import urllib2
import httplib
import re
import requests
import threading

def getLinks(url, pages, base, parentUrl=""):

    try:
        if not isValidLink(url):
            return pages

        # get page
        html_page = unicode(urllib2.urlopen(url).read(), errors='ignore')
    except httplib.InvalidURL, error:
        print "\n" + str(error) + " -> " + url + " (Found in "+parentUrl+")"
        sys.exit("Invalid URL" + url)
    except urllib2.HTTPError, error:
        if 508 != error.getcode():
            print str(error) + " -> " + url
            sys.exit("HTTP error on " + url)
        return pages
    except urllib2.URLError, error:
        print "\n" + str(error) + " -> " + url + " (Found in "+parentUrl+")"
        sys.exit("URL error on " + url)
    except ValueError, error:
        print "\n" + str(error) + " -> " + url + " (Found in "+parentUrl+")"
        sys.exit("Value error on " + url)
    except AttributeError, error:
        print "\n" + str(error) + " -> " + url + " (Found in "+parentUrl+")"
        sys.exit("Attribute error on " + url)

    try:
        soup = BeautifulSoup(html_page)
    except UnicodeEncodeError, error:
        # Pdf file
        sys.exit("UnicodeEncodeError error on " + url)


    links = soup.findAll('a')
    # foreach link in page
    for link in links:
        path = link.get('href')
        if path == None:
            continue

        path = formatUrl(path, base)
        domain = urlparse(path).hostname
        if domain != None and domain in base and path not in pages and path != None:
            file = open("log.txt", "a")
            file.write(path + "\n")
            pages.append(path)
            thread = threading.Thread(target=getLinks, args=(path,pages,base, url))
            thread.start()

    return pages

def isValidLink(url):
    extensions = ['.jpg', '.png', '.pdf', 'tel:', 'mailto:', 'javascript:']
    for extension in extensions:
        if extension in url:
            return False

    return True

def formatUrl(path, base):
    try:
        if path[0] == '/' and path[1] == '/':
            scheme = urlparse(base).scheme
            path = scheme + ":" + path
        if path[0] == '/':
            path = base + path
    except IndexError:
        return base
    if not base in path and not 'http' in path:
        path = base + '/' + path
    return path

# Expecting URL as parameter
if len(sys.argv) != 2:
    print 'File path needed'
    sys.exit(0)

url = sys.argv[1]
pages = []
file = open("log.txt", "w")
getLinks(url, pages, url)
